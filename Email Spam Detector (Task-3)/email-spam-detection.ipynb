{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **EMAIL SPAM DETECTION WITH MACHINE LEARNING (TASK-3)**\nProject By: **Tejaswini Jaunjat**\n\nWe’ve all been the recipient of spam emails before. Spam mail, or junk mail, is a type of email\nthat is sent to a massive number of users at one time, frequently containing cryptic\nmessages, scams, or most dangerously, phishing content.\n\n\n\nIn this Project, I used Python to build an email spam detector. Then, use machine learning to\ntrain the spam detector to recognize and classify emails into spam and non-spam. Let’s get\nstarted!","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Load the dataset into a pandas DataFrame\ndf = pd.read_csv('/kaggle/input/spam-dataset/spam.csv', encoding='latin-1')\n\n# Drop unnecessary columns and rename remaining columns\ndf = df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1)\ndf = df.rename(columns={'v1': 'label', 'v2': 'text'})\n\n# Print the first few rows of the DataFrame\nprint(df.head())\n","metadata":{"execution":{"iopub.status.busy":"2023-04-12T10:50:46.817618Z","iopub.execute_input":"2023-04-12T10:50:46.818030Z","iopub.status.idle":"2023-04-12T10:50:46.890624Z","shell.execute_reply.started":"2023-04-12T10:50:46.817995Z","shell.execute_reply":"2023-04-12T10:50:46.889468Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"  label                                               text\n0   ham  Go until jurong point, crazy.. Available only ...\n1   ham                      Ok lar... Joking wif u oni...\n2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n3   ham  U dun say so early hor... U c already then say...\n4   ham  Nah I don't think he goes to usf, he lives aro...\n","output_type":"stream"}]},{"cell_type":"markdown","source":"This code will load the spam.csv file into a pandas DataFrame, drop unnecessary columns, and rename the remaining columns to 'label' and 'text'. The 'label' column contains the labels (spam or ham) and the 'text' column contains the text messages.\n\nOnce we have loaded the data, we can start processing the text messages and extracting features from them. One approach is to use the Bag of Words (BoW) model to convert the text messages into numerical feature vectors.","metadata":{}},{"cell_type":"markdown","source":"**We can use the CountVectorizer class from scikit-learn to implement the BoW model. Here's some sample code:**","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\n# Create a CountVectorizer object\ncount_vectorizer = CountVectorizer()\n\n# Fit and transform the text data\ncount_vectorizer.fit(df['text'])\ntext_counts = count_vectorizer.transform(df['text'])\n\n# Print the shape of the feature vectors\nprint(text_counts.shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-12T10:52:18.778036Z","iopub.execute_input":"2023-04-12T10:52:18.778513Z","iopub.status.idle":"2023-04-12T10:52:19.437854Z","shell.execute_reply.started":"2023-04-12T10:52:18.778475Z","shell.execute_reply":"2023-04-12T10:52:19.436636Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"(5572, 8672)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"This code will create a CountVectorizer object and fit it to the text data. It will then transform the text data into feature vectors using the transform() method of the CountVectorizer object. Finally, it will print the shape of the feature vectors.\n\nNow that we have extracted features from the text messages, we can split the dataset into training and testing sets and train a machine learning model on the training set. We can use the Multinomial Naive Bayes (MNB) algorithm, which is a popular algorithm for text classification tasks.","metadata":{}},{"cell_type":"markdown","source":"**Here's some sample code to split the dataset, train the MNB model, and test it on the testing set:**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(text_counts, df['label'], test_size=0.2, random_state=42)\n\n# Train the MNB model\nmnb = MultinomialNB()\nmnb.fit(X_train, y_train)\n\n# Make predictions on the testing set\ny_pred = mnb.predict(X_test)\n\n# Evaluate the accuracy of the model\naccuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy:', accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-12T10:54:14.919155Z","iopub.execute_input":"2023-04-12T10:54:14.919667Z","iopub.status.idle":"2023-04-12T10:54:15.014442Z","shell.execute_reply.started":"2023-04-12T10:54:14.919629Z","shell.execute_reply":"2023-04-12T10:54:15.013222Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Accuracy: 0.97847533632287\n","output_type":"stream"}]},{"cell_type":"markdown","source":"This code will split the dataset into training and testing sets, train the MNB model on the training set, make predictions on the testing set, and evaluate the accuracy of the model. The accuracy score will give us an idea of how well the model is able to classify the text messages as spam or ham.","metadata":{}},{"cell_type":"markdown","source":"# **Data Cleaning and Preprocessing**\nBefore training a machine learning model, it's important to clean and preprocess the data. Here's some sample code that you can use to perform some basic data cleaning and preprocessing on the SMS spam collection dataset:","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\n\n# Load the dataset into a pandas DataFrame\ndf = pd.read_csv('/kaggle/input/spam-dataset/spam.csv', encoding='latin-1')\n\n# Drop unnecessary columns and rename remaining columns\ndf = df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1)\ndf = df.rename(columns={'v1': 'label', 'v2': 'text'})\n\n# Clean and preprocess the text data\nstop_words = set(stopwords.words('english'))\nstemmer = PorterStemmer()\n\ndef clean_text(text):\n    # Remove non-alphanumeric characters\n    text = re.sub('[^a-zA-Z0-9]', ' ', text)\n    \n    # Convert to lowercase\n    text = text.lower()\n    \n    # Remove stopwords\n    text = ' '.join([word for word in text.split() if word not in stop_words])\n    \n    # Stem words\n    text = ' '.join([stemmer.stem(word) for word in text.split()])\n    \n    return text\n\ndf['text'] = df['text'].apply(clean_text)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-12T10:55:51.748413Z","iopub.execute_input":"2023-04-12T10:55:51.749067Z","iopub.status.idle":"2023-04-12T10:55:53.136392Z","shell.execute_reply.started":"2023-04-12T10:55:51.749013Z","shell.execute_reply":"2023-04-12T10:55:53.135038Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"This code will load the spam.csv file into a pandas DataFrame, drop unnecessary columns, and rename the remaining columns to 'label' and 'text'. It will then clean and preprocess the text data by removing non-alphanumeric characters, converting to lowercase, removing stopwords, and stemming words using the PorterStemmer algorithm from the NLTK library.","metadata":{}},{"cell_type":"markdown","source":"# **Feature Engineering**\nIn addition to the Bag of Words model, there are many other features that you can extract from the text messages. Here's some sample code that you can use to extract additional features:","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nimport numpy as np\n\n# Create a TfidfVectorizer object\ntfidf_vectorizer = TfidfVectorizer()\n\n# Fit and transform the text data\ntfidf_vectorizer.fit(df['text'])\ntext_tfidf = tfidf_vectorizer.transform(df['text'])\n\n# Create a HashingVectorizer object\nhash_vectorizer = HashingVectorizer(n_features=2**10)\n\n# Fit and transform the text data\nhash_vectorizer.fit(df['text'])\ntext_hash = hash_vectorizer.transform(df['text'])\n\n# Create a TruncatedSVD object\nsvd = TruncatedSVD(n_components=100)\n\n# Fit and transform the tf-idf feature vectors\ntext_svd = svd.fit_transform(text_tfidf)\n\n# Concatenate the feature vectors\ntext_features = np.hstack((text_tfidf.toarray(), text_hash.toarray(), text_svd))\n","metadata":{"execution":{"iopub.status.busy":"2023-04-12T10:56:56.678986Z","iopub.execute_input":"2023-04-12T10:56:56.680443Z","iopub.status.idle":"2023-04-12T10:56:58.062051Z","shell.execute_reply.started":"2023-04-12T10:56:56.680388Z","shell.execute_reply":"2023-04-12T10:56:58.060769Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"This code will create a TfidfVectorizer object to extract TF-IDF features from the text messages, a HashingVectorizer object to extract hashing features, and a TruncatedSVD object to perform dimensionality reduction on the TF-IDF feature vectors. It will then concatenate the feature vectors into a single numpy array.","metadata":{}},{"cell_type":"markdown","source":"# **Hyperparameter Tuning**\nTo improve the performance of the machine learning model, you can tune the hyperparameters of the algorithm. Here's some sample code that you can use to perform hyperparameter tuning:","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n\n# Create a Naive Bayes model\nmodel = MultinomialNB()\n\n# Train the model on the training data\nmodel.fit(X_train, y_train)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-12T11:03:22.338441Z","iopub.execute_input":"2023-04-12T11:03:22.338898Z","iopub.status.idle":"2023-04-12T11:03:22.365238Z","shell.execute_reply.started":"2023-04-12T11:03:22.338860Z","shell.execute_reply":"2023-04-12T11:03:22.364045Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"MultinomialNB()"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n# Define the parameter grid\nparam_grid = {'alpha': [0.1, 1.0, 10.0]}\n\n# Perform grid search with cross-validation\ngrid_search = GridSearchCV(model, param_grid=param_grid, cv=5, n_jobs=-1)\n\n# Fit the model to the data\ngrid_search.fit(X_train, y_train)\n\n# Print the best parameters and score\nprint('Best parameters:', grid_search.best_params_)\nprint('Best score:', grid_search.best_score_)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T11:03:28.363521Z","iopub.execute_input":"2023-04-12T11:03:28.363971Z","iopub.status.idle":"2023-04-12T11:03:30.096132Z","shell.execute_reply.started":"2023-04-12T11:03:28.363931Z","shell.execute_reply":"2023-04-12T11:03:30.094556Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Best parameters: {'alpha': 0.1}\nBest score: 0.9833972510355171\n","output_type":"stream"}]},{"cell_type":"markdown","source":"This code defines the parameter grid for hyperparameter tuning. In this case, we are testing different values for the alpha hyperparameter, which controls the strength of regularization in the Naive Bayes algorithm. We are testing the values 0.1, 1.0, and 10.0. We can add more values to this list if we want to test additional values.\nThis code will perform a grid search with cross-validation to find the best hyperparameters for the algorithm. It will print out the best parameters and score.","metadata":{}},{"cell_type":"markdown","source":"# **Model Evaluation**\nAfter training the machine learning model, it's important to evaluate its performance. Here's some sample code that you can use to evaluate the model:","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# Make predictions on the test data\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nprint('Accuracy:', accuracy_score(y_test, y_pred))\nprint('Precision:', precision_score(y_test, y_pred, pos_label='spam'))\nprint('Recall:', recall_score(y_test, y_pred, pos_label='spam'))\nprint('F1 score:', f1_score(y_test, y_pred, pos_label='spam'))\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-12T11:05:56.758702Z","iopub.execute_input":"2023-04-12T11:05:56.759361Z","iopub.status.idle":"2023-04-12T11:05:56.791791Z","shell.execute_reply.started":"2023-04-12T11:05:56.759291Z","shell.execute_reply":"2023-04-12T11:05:56.790492Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Accuracy: 0.97847533632287\nPrecision: 0.9144736842105263\nRecall: 0.9266666666666666\nF1 score: 0.9205298013245033\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Thank you**","metadata":{}}]}